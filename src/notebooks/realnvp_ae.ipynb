{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c238fa5",
   "metadata": {},
   "source": [
    "Testing out a RealNVP normalising flow with a ResNet autoencoder to model the latent space of images as a multivariate normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1442ab38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel/miniconda3/envs/normflow/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Flow(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        raise NotImplementedError(\"Forward pass not implemented\")\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        raise NotImplementedError(\"Inverse pass not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1aa6dd",
   "metadata": {},
   "source": [
    "Implement a masked affine coupling layer that operates directly on the latent vector produced by ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bddc560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentMaskedAffineCoupling(Flow):\n",
    "    def __init__(self, b, net):\n",
    "        super().__init__()\n",
    "        self.register_buffer('b', b)\n",
    "        self.net = net\n",
    "        self.scaling = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, z):\n",
    "        z_masked = self.b * z\n",
    "        s, t = self.net(z_masked).chunk(2, dim=1)\n",
    "\n",
    "        s_exp = self.scaling.exp()\n",
    "        s = torch.tanh(s / s_exp) * s_exp\n",
    "\n",
    "        z_out = z_masked + (1 - self.b) * (z * torch.exp(s) + t)\n",
    "        log_det = torch.sum((1 - self.b) * s, dim=list(range(1, self.b.dim())))\n",
    "        return z_out, log_det\n",
    "\n",
    "    def inverse(self, z):\n",
    "        z_masked = self.b * z\n",
    "        s, t = self.net(z_masked).chunk(2, dim=1)\n",
    "        \n",
    "        s_exp = self.scaling.exp()\n",
    "        s = torch.tanh(s / s_exp) * s_exp\n",
    "\n",
    "        z_out = z_masked + (1 - self.b) * (z - t) * torch.exp(-s)\n",
    "        log_det = -torch.sum((1 - self.b) * s, dim=list(range(1, self.b.dim())))\n",
    "        return z_out, log_det\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661770d",
   "metadata": {},
   "source": [
    "Implement a normalising flow class that wraps around pre-specified flow layers, and allows us to do forward passes (image latent vector -> target distribution, in this case multivariate normal), as well as to sample (sample from target distribution -> image latent vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6541d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalisingFlow(nn.Module):\n",
    "    def __init__(self, flows, prior, device):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "        self.prior = prior # Target distribution to be approximated\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, z):\n",
    "        log_det = torch.zeros(z.shape, device=self.device)\n",
    "        for flow in self.flows:\n",
    "            z, local_log_det = flow(z)\n",
    "            log_det += local_log_det\n",
    "        return z, log_det\n",
    "\n",
    "    def sample(self, sample_shape=None):\n",
    "        if sample_shape:\n",
    "            z = self.prior.sample(sample_shape=sample_shape)\n",
    "        else:\n",
    "            z = self.prior.sample() # No need to specify for multivariate normal\n",
    "\n",
    "        z = z.to(self.device)\n",
    "        for flow in reversed(self.flows):\n",
    "            z, _ = flow.inverse(z)\n",
    "        return z\n",
    "    \n",
    "    def get_prior_log_prob(self, z):\n",
    "        return self.prior.log_prob(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad6c32",
   "metadata": {},
   "source": [
    "Set up neural network classes needed for the flow layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b474f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        if num_layers == 1:\n",
    "            layers = [nn.Linear(in_size, 2*in_size)]\n",
    "        else:\n",
    "            layers = [nn.Linear(in_size, hidden_size, bias=True), nn.LeakyReLU()]\n",
    "            for i in range(num_layers - 2):\n",
    "                layers += [nn.Linear(hidden_size, hidden_size, bias=True), nn.LeakyReLU()]\n",
    "            layers += [nn.Linear(hidden_size, 2*in_size, bias=True)]\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf92ce",
   "metadata": {},
   "source": [
    "Set up the models and the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "167a5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bottleneck = 256\n",
    "n_flows = 16\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f43148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cifar10-resnet18']\n",
      "['cifar10-resnet18', 'stl10-resnet18']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'resnet50-imagenet not present in pretrained weights.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ceff44f8f20d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_weights_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_weights_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet50-imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# ae.freeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ae = ae.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/normflow/lib/python3.7/site-packages/pl_bolts/models/autoencoders/basic_ae/basic_ae_module.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, checkpoint_name)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not present in pretrained weights.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_urls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'resnet50-imagenet not present in pretrained weights.'"
     ]
    }
   ],
   "source": [
    "from pl_bolts.models.autoencoders import AE, VAE\n",
    "ae = AE(input_height=32, enc_type='resnet50')\n",
    "print(AE.pretrained_weights_available())\n",
    "print(VAE.pretrained_weights_available())\n",
    "ae = ae.from_pretrained('resnet50-imagenet')\n",
    "# ae.freeze()\n",
    "# ae = ae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da5a2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the flow layers\n",
    "b = torch.tensor(n_bottleneck // 2 * [0, 1] + n_bottleneck % 2 * [0])\n",
    "flows = []\n",
    "for i in range(n_flows):\n",
    "    st_net = MLP(n_bottleneck, 1024, 3)\n",
    "    if i % 2 == 0:\n",
    "        flows += [LatentMaskedAffineCoupling(b, st_net)]\n",
    "    else:\n",
    "        flows += [LatentMaskedAffineCoupling(1 - b, st_net)]\n",
    "\n",
    "# Specify the prior (target distribution)\n",
    "# prior = torch.distributions.MultivariateNormal(\n",
    "#     torch.zeros(n_bottleneck, device=device),\n",
    "#     torch.eye(n_bottleneck, device=device)\n",
    "#     )\n",
    "prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n",
    "        \n",
    "# Construct the normalising flow\n",
    "nf = NormalisingFlow(flows, prior, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995070d6",
   "metadata": {},
   "source": [
    "Load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef4b6a52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-eb03cf3c539a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Transformations applied on each image => make them a tensor and discretize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Loading the training dataset. We need to split it into a training and validation part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../../data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# Transformations applied on each image => make them a tensor and discretize\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = datasets.CIFAR10(root=\"../../data\", train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [42000, 8000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = datasets.CIFAR10(root=\"../../data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Define the data loaders. We currently do not use the validation and test sets.\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "print(\"Loaded in data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a4d70",
   "metadata": {},
   "source": [
    "Training the normalising flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce3dd762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:25<00:00, 26.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [10496/42000 (100%)]\tLoss: -20935.705078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:24<00:00, 26.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [10496/42000 (100%)]\tLoss: -21519.406250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:25<00:00, 25.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [10496/42000 (100%)]\tLoss: -22077.816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:26<00:00, 25.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [10496/42000 (100%)]\tLoss: -22423.375000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:26<00:00, 25.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [10496/42000 (100%)]\tLoss: -22700.769531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:25<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [10496/42000 (100%)]\tLoss: -22848.933594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:24<00:00, 26.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [10496/42000 (100%)]\tLoss: -23033.564453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:24<00:00, 26.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [10496/42000 (100%)]\tLoss: -23308.318359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:24<00:00, 26.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [10496/42000 (100%)]\tLoss: -23441.519531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:24<00:00, 26.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [10496/42000 (100%)]\tLoss: -23752.937500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer =  torch.optim.Adam(nf.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "for epoch in range(n_epochs):\n",
    "    progressbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_n, (x, n) in progressbar:\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = ae.encoder(x)\n",
    "        z = ae.fc(z)\n",
    "        z, log_det = nf(z)\n",
    "        log_prob_z = nf.get_prior_log_prob(z)\n",
    "        loss = -log_prob_z.mean() - log_det.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progressbar.update()\n",
    "    progressbar.close()\n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_n * len(x), len(train_loader.dataset),\n",
    "                       100. * batch_n / len(train_loader),\n",
    "                       loss.item()))\n",
    "    \n",
    "torch.save(nf.state_dict(), \"../../saved_models/realnvp_16l_10p.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd872e7",
   "metadata": {},
   "source": [
    "Sampling from the trained normalising flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c91b5a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = nf.sample(sample_shape=[2, 256])\n",
    "recon = ae.decoder(samples)\n",
    "x_np = recon.view((2, 3, 32, 32)).to('cpu').detach().numpy()\n",
    "x0 = x_np[0].swapaxes(0, 2).swapaxes(0, 1)\n",
    "plt.imshow(x0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4fdcb5",
   "metadata": {},
   "source": [
    "Compare the results with an untrained normalising flow to see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30509069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the flow layers\n",
    "b_clean = torch.tensor(n_bottleneck // 2 * [0, 1] + n_bottleneck % 2 * [0])\n",
    "flows_clean = []\n",
    "for i in range(n_flows):\n",
    "    st_net_clean = MLP(n_bottleneck, 1024, 2)\n",
    "    if i % 2 == 0:\n",
    "        flows_clean += [LatentMaskedAffineCoupling(b_clean, st_net_clean)]\n",
    "    else:\n",
    "        flows_clean += [LatentMaskedAffineCoupling(1 - b_clean, st_net_clean)]\n",
    "\n",
    "prior_clean = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n",
    "        \n",
    "# Construct the normalising flow\n",
    "nf_clean = NormalisingFlow(flows_clean, prior_clean, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59998fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = nf_clean.sample(sample_shape=[2, 256])\n",
    "recon = ae.decoder(samples)\n",
    "x_np = recon.view((2, 3, 32, 32)).to('cpu').detach().numpy()\n",
    "x0 = x_np[0].swapaxes(0, 2).swapaxes(0, 1)\n",
    "plt.imshow(x0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:normflow] *",
   "language": "python",
   "name": "conda-env-normflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
