{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c238fa5",
   "metadata": {},
   "source": [
    "Testing out variants of autoencoders for anomaly detection. We achieve this by using the autoencoders to reconstruct images of the scene, and computing the reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1442ab38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel/miniconda3/envs/normflow/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Flow(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        raise NotImplementedError(\"Forward pass not implemented\")\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        raise NotImplementedError(\"Inverse pass not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1aa6dd",
   "metadata": {},
   "source": [
    "Implement a masked affine coupling layer that operates directly on the latent vector produced by ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bddc560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentMaskedAffineCoupling(Flow):\n",
    "    def __init__(self, b, net):\n",
    "        super().__init__()\n",
    "        self.register_buffer('b', b)\n",
    "        self.net = net\n",
    "        self.scaling = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, z):\n",
    "        z_masked = self.b * z\n",
    "        s, t = self.net(z_masked).chunk(2, dim=1)\n",
    "\n",
    "        s_exp = self.scaling.exp()\n",
    "        s = torch.tanh(s / s_exp) * s_exp\n",
    "\n",
    "        z_out = z_masked + (1 - self.b) * (z * torch.exp(s) + t)\n",
    "        log_det = torch.sum((1 - self.b) * s, dim=list(range(1, self.b.dim())))\n",
    "        return z_out, log_det\n",
    "\n",
    "    def inverse(self, z):\n",
    "        z_masked = self.b * z\n",
    "        s, t = self.net(z_masked).chunk(2, dim=1)\n",
    "        \n",
    "        s_exp = self.scaling.exp()\n",
    "        s = torch.tanh(s / s_exp) * s_exp\n",
    "\n",
    "        z_out = z_masked + (1 - self.b) * (z - t) * torch.exp(-s)\n",
    "        log_det = -torch.sum((1 - self.b) * s, dim=list(range(1, self.b.dim())))\n",
    "        return z_out, log_det\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661770d",
   "metadata": {},
   "source": [
    "Implement a normalising flow class that wraps around pre-specified flow layers, and allows us to do forward passes (image latent vector -> target distribution, in this case multivariate normal), as well as to sample (sample from target distribution -> image latent vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6541d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalisingFlow(nn.Module):\n",
    "    def __init__(self, flows, prior, device):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "        self.prior = prior # Target distribution to be approximated\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, z):\n",
    "        log_det = torch.zeros(z.shape, device=self.device)\n",
    "        for flow in self.flows:\n",
    "            z, local_log_det = flow(z)\n",
    "            log_det += local_log_det\n",
    "        return z, log_det\n",
    "\n",
    "    def sample(self, sample_shape=None):\n",
    "        if sample_shape:\n",
    "            z = self.prior.sample(sample_shape=sample_shape)\n",
    "        else:\n",
    "            z = self.prior.sample() # No need to specify for multivariate normal\n",
    "\n",
    "        z = z.to(self.device)\n",
    "        for flow in reversed(self.flows):\n",
    "            z, _ = flow.inverse(z)\n",
    "        return z\n",
    "    \n",
    "    def get_prior_log_prob(self, z):\n",
    "        return self.prior.log_prob(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad6c32",
   "metadata": {},
   "source": [
    "Set up neural network classes needed for the flow layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b474f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        if num_layers == 1:\n",
    "            layers = [nn.Linear(in_size, 2*in_size)]\n",
    "        else:\n",
    "            layers = [nn.Linear(in_size, hidden_size, bias=True), nn.LeakyReLU()]\n",
    "            for i in range(num_layers - 2):\n",
    "                layers += [nn.Linear(hidden_size, hidden_size, bias=True), nn.LeakyReLU()]\n",
    "            layers += [nn.Linear(hidden_size, 2*in_size, bias=True)]\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a225c",
   "metadata": {},
   "source": [
    "Implements a convolutional autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c678a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34d90066",
   "metadata": {},
   "source": [
    "Implements building blocks like encoders and decoders, that can be use as part of a VAE or a VAE + NF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61c45ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarEncoder(nn.Module):\n",
    "    def __init__(self, latent_size=64, kernel_size=5, img_height=32, channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, kernel_size),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        # Calculate the output size of the image tensor after 4 convolution layers\n",
    "        out_h = img_height\n",
    "        out_w = img_height\n",
    "        for i in range(4):\n",
    "            out_h, out_w = self.convOutputShape((out_h, out_w), kernel_size)\n",
    "        \n",
    "        self.flattened_size = out_h * out_w * 128\n",
    "        self.latent_img_height = out_h\n",
    "        \n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, latent_size, bias=True)\n",
    "        )\n",
    "        \n",
    "        self.fc_sigma = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, latent_size, bias=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.conv(x).view(x.shape[0], -1)\n",
    "        mu = self.fc_mu(z)\n",
    "        sigma = torch.exp(self.fc_sigma(z))\n",
    "        return mu, sigma\n",
    "    \n",
    "    def convOutputShape(self, h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "        if type(kernel_size) is not tuple:\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        h = math.floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "        w = math.floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "        return h, w\n",
    "    \n",
    "class VarDecoder(nn.Module):\n",
    "    def __init__(self, flattened_size, latent_img_height, latent_size=64, kernel_size=5, channels=3):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_size, flattened_size)\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(32, channels, kernel_size)\n",
    "        )\n",
    "        self.latent_img_height = latent_img_height\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.fc(x).view(x.shape[0], 128, self.latent_img_height, self.latent_img_height)\n",
    "        im = self.conv(z)\n",
    "        return im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f17b00",
   "metadata": {},
   "source": [
    "Set up classes that put the previous components together into the form of a VAE, and VAE + NF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c3127e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, device, flows = None, latent_size=64, kernel_size=5, img_height=32, channels=3):\n",
    "        super().__init__()\n",
    "        self.encoder = VarEncoder(latent_size, kernel_size, img_height, channels)\n",
    "        self.decoder = VarDecoder(self.encoder.flattened_size, self.encoder.latent_img_height)\n",
    "        self.device = device\n",
    "        self.flows = flows\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encoder(x)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        im = self.decoder(z)\n",
    "        return im, mu, sigma\n",
    "    \n",
    "    def reparameterize(self, mu, sigma):\n",
    "        epsilon = torch.randn_like(mu, device=self.device)\n",
    "        z = mu + sigma * epsilon\n",
    "        return z\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # Sample from N(0, 1)\n",
    "        z = torch.randn((batch_size, self.latent_size), device=self.device)\n",
    "        if self.flows is not None:\n",
    "            z = self.flows(z)\n",
    "        im = self.decoder(z)\n",
    "        return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fe1388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "net3 = VariationalAutoencoder(device).to(device)\n",
    "im, _, _ = net3(torch.zeros(2, 3, 32, 32).to(device))\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf92ce",
   "metadata": {},
   "source": [
    "Set up the models, hyperparameters and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "167a5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4af04c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo_loss(x, y, mu, sigma):\n",
    "    kld = -0.5 * torch.sum(1 + torch.log(sigma) - mu.pow(2) - sigma)\n",
    "    #recon_loss = nn.functional.binary_cross_entropy(x, y, reduction='sum')\n",
    "    recon_loss = torch.sum((x - y)**2)\n",
    "    return recon_loss + kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff6a66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VariationalAutoencoder(device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995070d6",
   "metadata": {},
   "source": [
    "Load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef4b6a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loaded in data.\n"
     ]
    }
   ],
   "source": [
    "# Transformations applied on each image => make them a tensor and discretize\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = datasets.CIFAR10(root=\"../../nf_tutorial/data\", train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [42000, 8000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = datasets.CIFAR10(root=\"../../nf_tutorial/data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Define the data loaders. We currently do not use the validation and test sets.\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "print(\"Loaded in data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a4d70",
   "metadata": {},
   "source": [
    "Training the normalising flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce3dd762",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:21<00:00, 31.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [10496/42000 (100%)]\tLoss: 2709.291504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:20<00:00, 31.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [10496/42000 (100%)]\tLoss: 2299.890137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:21<00:00, 30.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [10496/42000 (100%)]\tLoss: 1808.630371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:20<00:00, 31.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [10496/42000 (100%)]\tLoss: 1545.900879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:20<00:00, 31.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [10496/42000 (100%)]\tLoss: 1391.030273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:20<00:00, 31.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [10496/42000 (100%)]\tLoss: 1339.552734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:20<00:00, 31.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [10496/42000 (100%)]\tLoss: 1272.291748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:21<00:00, 30.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [10496/42000 (100%)]\tLoss: 1247.435059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:20<00:00, 31.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [10496/42000 (100%)]\tLoss: 1229.814453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:20<00:00, 31.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [10496/42000 (100%)]\tLoss: 1187.127197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:21<00:00, 31.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [10496/42000 (100%)]\tLoss: 1168.813721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:20<00:00, 31.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [10496/42000 (100%)]\tLoss: 1132.734497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:20<00:00, 31.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [10496/42000 (100%)]\tLoss: 1117.435303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:22<00:00, 28.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [10496/42000 (100%)]\tLoss: 1090.218750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:22<00:00, 28.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [10496/42000 (100%)]\tLoss: 1083.170166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:22<00:00, 29.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [10496/42000 (100%)]\tLoss: 1068.625488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:21<00:00, 31.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [10496/42000 (100%)]\tLoss: 1058.733154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:21<00:00, 31.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [10496/42000 (100%)]\tLoss: 1061.038208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:21<00:00, 30.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18 [10496/42000 (100%)]\tLoss: 1075.671997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:21<00:00, 31.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [10496/42000 (100%)]\tLoss: 1033.724243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-e444cffb17f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                        loss.item()))\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../../nf_tutorial/saved_models/vae.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nf' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer =  torch.optim.Adam(vae.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "for epoch in range(n_epochs):\n",
    "    progressbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_n, (x, n) in progressbar:\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, mu, sigma = vae(x)\n",
    "        #loss = elbo_loss(x.detach(), outputs.detach(), mu.detach(), sigma.detach())\n",
    "        loss = elbo_loss(x, outputs, mu, sigma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progressbar.update()\n",
    "    progressbar.close()\n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_n * len(x), len(train_loader.dataset),\n",
    "                       100. * batch_n / len(train_loader),\n",
    "                       loss.item()))\n",
    "    \n",
    "torch.save(vae.state_dict(), \"../../nf_tutorial/saved_models/vae.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd872e7",
   "metadata": {},
   "source": [
    "Sampling from the trained normalising flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a09fad80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = vae.sample(2)\n",
    "ims = samples.to('cpu').detach().numpy()\n",
    "im0 = ims[0].swapaxes(0, 2).swapaxes(0, 1)\n",
    "plt.imshow(im0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5cbd0",
   "metadata": {},
   "source": [
    "Test against a clean VAE that has not been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c91b5a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae_clean = VariationalAutoencoder(device).to(device)\n",
    "samples = vae_clean.sample(2)\n",
    "ims = samples.to('cpu').detach().numpy()\n",
    "im0 = ims[0].swapaxes(0, 2).swapaxes(0, 1)\n",
    "\n",
    "plt.imshow(im0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:normflow] *",
   "language": "python",
   "name": "conda-env-normflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
